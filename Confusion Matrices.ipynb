{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Validation\n",
    "Not all dictionaries are made the same, which is why we are validating them pairwise for each source. Hard to say why analysis yields different results based on the dictionary. Maybe it has to do with the language that each individual magazine uses. For instance, the conservative Deutschland Kurier uses sarcasm a lot. It may wax positive about an event but does so to make fun of it and expose the delusion of Germans and German authorities. In this case, the positive lexicon may mislead the algorithm and make it inclined to mark a text as \"positive.\" Therefore, it may be a better idea to use a dictionary with a greater emphasis on negative vocabulary when processing such corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pandas import read_csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1. For the purposes of this research, we needed to be able to compare the means of sentiment scores. Standardization would not work as it sets a mean to zero. Besides, a standard deviation can tell us a lot about the diversity of scores. So making it one again would deprove us of some very useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.7623471473248491, pvalue=0.4465232642872813)\n"
     ]
    }
   ],
   "source": [
    "df_taz_sentiws = pd.read_excel('Taz Sentiment.xlsx')\n",
    "df_taz_zurich = pd.read_excel('Taz_Sentiment_Zurich.xlsx')\n",
    "\n",
    "#list of values from the Zurich dictionary\n",
    "#does not need normalization as they lie between 0 and 1\n",
    "data_taz_Zu = df_taz_zurich['Value'].tolist()\n",
    "\n",
    "\n",
    "#normalization of the SW dictionary:\n",
    "#reshaping into a vector, normalizing, converting back to the list form, \n",
    "#and getting rid of parentheses\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "data_taz_WS = df_taz_sentiws['Value'].tolist()\n",
    "data_taz_WS = np.reshape(data_taz_WS, (-1,1))\n",
    "data_scaled_WS = scaler.fit_transform(data_taz_WS)\n",
    "data_scaled_WS = data_scaled_WS.tolist()\n",
    "data_scaled_WS = [item for sublist in data_scaled_WS for item in sublist]\n",
    "\n",
    "print(stats.ttest_ind(data_scaled_WS, data_taz_Zu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ttest_indResult(statistic=0.7623471473248491, pvalue=0.4465232642872813)\n",
    "\n",
    "Student's T-test shows that there is a significant difference between the means of two groups. It means that after normalization, the scores yielded with SentiWS and PolartLexicon are more or less similar. But do they prove to be true after two humans (me and a native speaker) make judgments about same texts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_test(df1,df2):\n",
    "    \"\"\"fisher's exact test is calculating the differences\n",
    "    between the results of classification derived from two different dictionaries\"\"\"\n",
    "    \n",
    "    list_of_numbers_for_ws = df1['Judgment'].value_counts().tolist()\n",
    "    list_of_numbers_for_zu = df2['Judgment'].value_counts().tolist()\n",
    "\n",
    "    oddsratio, pvalue = stats.fisher_exact([[list_of_numbers_for_ws[0], list_of_numbers_for_taz_zu[0]],\n",
    "                                        [list_of_numbers_for_ws[1], list_of_numbers_for_zu[1]]]) \n",
    "    return oddsratio, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taz (1.0054171180931744, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Taz\", fisher_test(df_taz_sentiws, df_taz_zurich))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taz oddsratio = 1.0054171180931744, p-value = 1.0\n",
    "\n",
    "Fisher's exact test is a statistical test used to determine if there are nonrandom associations between two or more categorical variables. Our sample is rather modest, so its use is justified. As you can see, there is no association between the numbers of negative, positive, and neutral scores yielded with the two dictionaries.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  4]\n",
      " [ 1  1  2]\n",
      " [ 1  0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative      0.833     0.714     0.769        14\n",
      "     Neutral      1.000     0.250     0.400         4\n",
      "    Positive      0.727     0.941     0.821        17\n",
      "\n",
      "    accuracy                          0.771        35\n",
      "   macro avg      0.854     0.635     0.663        35\n",
      "weighted avg      0.801     0.771     0.752        35\n",
      "\n",
      "[[10  0  0]\n",
      " [ 1  0  2]\n",
      " [ 2  0 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative      0.769     1.000     0.870        10\n",
      "     Neutral      0.000     0.000     0.000         3\n",
      "    Positive      0.882     0.882     0.882        17\n",
      "\n",
      "    accuracy                          0.833        30\n",
      "   macro avg      0.551     0.627     0.584        30\n",
      "weighted avg      0.756     0.833     0.790        30\n",
      "\n",
      "[[10  0  0]\n",
      " [ 3  0  0]\n",
      " [14  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative      0.370     1.000     0.541        10\n",
      "     Neutral      0.000     0.000     0.000         3\n",
      "    Positive      1.000     0.176     0.300        17\n",
      "\n",
      "    accuracy                          0.433        30\n",
      "   macro avg      0.457     0.392     0.280        30\n",
      "weighted avg      0.690     0.433     0.350        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wedan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrices for Taz.de\n",
    "#Actual is the result of manual validation\n",
    "\n",
    "P=\"Positive\"\n",
    "N=\"Negative\"\n",
    "O=\"Neutral\"\n",
    "\n",
    "taz_ws_actual = [N, P, N, P, N, O, P, P, P, O, P, N, N, O, P, N, P, O, P, N, N, N, P, P, P, P, N, P, N, P, P, N, N, N, P]\n",
    "taz_ws_pred = [N, P, P, N, N, P, P, P, P, O, P, N, N, P, P, P, P, N, P, P, P, N, P, P, P, P, N, P, N, P, P, N, N, N, P]\n",
    "\n",
    "taz_zu_actual = [N, P, N, P, N, O, P, P, P, O, P, N, N, O, P, N, P, P, P, P, N, N, P, P, P, P, P, N, N, P]\n",
    "taz_zu_pred = [N, P, N, N, N, P, P, P, N, N, P, N, N, P, P, N, P, P, P,P, N, N, P, P, P, P, P, N, N, P]\n",
    "taz_zu_pred_norm = [O, P, N, N, N, P, P, P, N, N, P, N, N, P, P, N, P, P, P, P, N, N, P, P, P, P, P, N, N, P] \n",
    "taz_hyb_pred = [N, N, N, N, N, N, N, P, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, P, N, N, N, P]\n",
    "taz_hyb_act = [N, P, N, P, N, O, P, P, P, O, P, N, N, O, P, N, P, P, P, P, N, N, P, P, P, P, P, N, N, P]\n",
    "\n",
    "\n",
    "print(metrics.confusion_matrix(taz_ws_actual,taz_ws_pred))\n",
    "print(metrics.classification_report(taz_ws_actual,taz_ws_pred, digits=3))\n",
    "\n",
    "print(metrics.confusion_matrix(taz_zu_actual,taz_zu_pred))\n",
    "print(metrics.classification_report(taz_zu_actual,taz_zu_pred, digits=3))\n",
    "\n",
    "print(metrics.confusion_matrix(taz_hyb_act,taz_hyb_pred))\n",
    "print(metrics.classification_report(taz_hyb_act,taz_hyb_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taz WS actual vs. Taz WS predicted gives us a macro averaged precision of 85.4%, which is pretty good for a rule-based sentiment analysis algorithm. \n",
    "\n",
    "\n",
    "#### SentiWS Confusion Matrix\n",
    "\n",
    "|            | precision | recall | f1-score | support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "|   Negative |      0.833|   0.714|    0.769 |      14 | \n",
    "     Neutral |     1.000 |   0.250|    0.400 |       4 |\n",
    "    Positive |     0.727 |   0.941|    0.821 |      17 |\n",
    "    accuracy |            |       |      0.771|      35\n",
    "   macro avg |     0.854  |   0.635 |    0.663 |       35\n",
    "weighted avg |     0.801  |   0.771 |    0.752 |       35\n",
    "\n",
    "\n",
    "PolartLexicon and the hybrid dictionary show very bad results, hence, should not be used any further. \n",
    "\n",
    "#### PolartLexicon Confusion Matrix\n",
    "\n",
    "|             | precision |   recall | f1-score |  support|\n",
    "|-------------|-----------|----------|----------|---------|\n",
    " |   Negative   |   0.769  |   1.000   |  0.870   |     10|\n",
    "     Neutral   |   0.000   |  0.000  |   0.000   |      3\n",
    "    Positive   |   0.882  |   0.882  |   0.882   |     17\n",
    "    accuracy   |           |          |  0.833   |     30\n",
    "   macro avg   |   0.551   |  0.627   |  0.584   |     30\n",
    "weighted avg   |   0.756   |  0.833   |  0.790   |     30\n",
    "\n",
    "#### Hybrid Confusion Matrix\n",
    "\n",
    "  |            |precision   | recall | f1-score |  support\n",
    "|---|---|---|---|---\n",
    "    Negative   |   0.370   |  1.000   |  0.541    |    10\n",
    "     Neutral    |  0.000    | 0.000  |  0.000      |   3\n",
    "    Positive     | 1.000     |0.176   |  0.300     |   17\n",
    "    accuracy      |         |         |  0.433      |  30\n",
    "   macro avg     | 0.457    |0.392    | 0.280      |  30\n",
    "weighted avg     | 0.690   | 0.433    | 0.350      |  30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_for_confmat(file, length):\n",
    "    \"\"\"this function returns a list of judgments \n",
    "    used in confusion matrices\"\"\"\n",
    "    \n",
    "    if file.endswith('xlsx'):\n",
    "        df = pd.read_excel(file)\n",
    "        df_slice = df.loc[:length]\n",
    "    else:\n",
    "        return 'Wrong format'\n",
    "    return df_slice['Judgment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  1  1]\n",
      " [ 3  0  3]\n",
      " [ 7  1  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.565     0.867     0.684        15\n",
      "     neutral      0.000     0.000     0.000         6\n",
      "    positive      0.333     0.200     0.250        10\n",
      "\n",
      "    accuracy                          0.484        31\n",
      "   macro avg      0.300     0.356     0.311        31\n",
      "weighted avg      0.381     0.484     0.412        31\n",
      "\n",
      "[[17  1  0]\n",
      " [ 0  1  0]\n",
      " [ 6  0  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.739     0.944     0.829        18\n",
      "     neutral      0.500     1.000     0.667         1\n",
      "    positive      1.000     0.500     0.667        12\n",
      "\n",
      "    accuracy                          0.774        31\n",
      "   macro avg      0.746     0.815     0.721        31\n",
      "weighted avg      0.832     0.774     0.761        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stern_zu_pred = list_for_confmat('Stern_Sentiment_Zurich.xlsx', 30)\n",
    "stern_ws_pred = list_for_confmat('Stern_Sentiment.xlsx', 30)\n",
    "\n",
    "stern_actual = ['negative', 'negative', 'positive', 'negative', 'negative', 'negative',\n",
    "               'negative', 'negative', 'negative', 'negative', 'negative', 'negative',\n",
    "               'negative', 'positive', 'negative', 'negative', 'negative', 'negative',\n",
    "               'negative', 'negative', 'negative', 'negative', 'positive', 'negative', \n",
    "               'neutral', 'positive', 'negative', 'negative', 'positive', 'neutral',\n",
    "               'positive']\n",
    "\n",
    "print(metrics.confusion_matrix(stern_ws_pred,stern_actual))\n",
    "print(metrics.classification_report(stern_ws_pred,stern_actual, digits=3))\n",
    "\n",
    "print(metrics.confusion_matrix(stern_zu_pred,stern_actual))\n",
    "print(metrics.classification_report(stern_zu_pred,stern_actual, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the social-democratic source, PolartLexicon with the accuracy of almost 80% seems to be the optimal choice. SentiWS is not much better than a flipping a coin, and the latter is arguably easier to implement :) \n",
    "\n",
    "#### SentiWS\n",
    "\n",
    " |             |precision |   recall|  f1-score  | support\n",
    "|---|---|---|---|---|\n",
    "    negative  |    0.565  |   0.867  |   0.684   |     15\n",
    "     neutral   |   0.000   |  0.000   |  0.000    |     6\n",
    "    positive    |  0.333    | 0.200   |  0.250     |   10\n",
    "    accuracy    |            |         | 0.484     |   31\n",
    "   macro avg     | 0.300     |0.356    | 0.311     |   31\n",
    "weighted avg      |0.381     |0.484    | 0.412     |   31\n",
    "\n",
    "\n",
    "#### PolartLexicon\n",
    "\n",
    "\n",
    "|             | precision  |  recall | f1-score |  support\n",
    "             |---|---|---|---|---|\n",
    "    negative  |    0.739 |    0.944  |   0.829    |    18\n",
    "     neutral   |   0.500 |    1.000   |  0.667    |     1\n",
    "    positive    |  1.000  |   0.500   |  0.667    |    12\n",
    "    accuracy   |           |          |  0.774    |    31\n",
    "   macro avg    |  0.746   |  0.815   |  0.721    |    31\n",
    "weighted avg    |  0.832   |  0.774   |  0.761    |    31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  1  0]\n",
      " [ 9  1  1]\n",
      " [ 6  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.444     0.923     0.600        13\n",
      "     neutral      0.333     0.091     0.143        11\n",
      "    positive      0.000     0.000     0.000         7\n",
      "\n",
      "    accuracy                          0.419        31\n",
      "   macro avg      0.259     0.338     0.248        31\n",
      "weighted avg      0.305     0.419     0.302        31\n",
      "\n",
      "[[10  1  0]\n",
      " [ 2  0  0]\n",
      " [15  2  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.370     0.909     0.526        11\n",
      "     neutral      0.000     0.000     0.000         2\n",
      "    positive      1.000     0.056     0.105        18\n",
      "\n",
      "    accuracy                          0.355        31\n",
      "   macro avg      0.457     0.322     0.211        31\n",
      "weighted avg      0.712     0.355     0.248        31\n",
      "\n",
      "[[16  2  0]\n",
      " [ 0  0  0]\n",
      " [11  1  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.593     0.889     0.711        18\n",
      "     neutral      0.000     0.000     0.000         0\n",
      "    positive      1.000     0.077     0.143        13\n",
      "\n",
      "    accuracy                          0.548        31\n",
      "   macro avg      0.531     0.322     0.285        31\n",
      "weighted avg      0.763     0.548     0.473        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wedan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dk_ws_pred = list_for_confmat('DK Sentiment.xlsx', 30)\n",
    "\n",
    "dk_zu_pred = list_for_confmat('DK_Sentiment_Zurich.xlsx', 30)\n",
    "dk_hyb_pred = list_for_confmat('Dk_Sentiment_Hybrid.xlsx', 30)\n",
    "\n",
    "dk_actual = ['negative', 'negative', 'negative', 'neutral', 'negative', 'negative', 'negative',\n",
    "            'negative', 'negative', 'neutral', 'negative', 'negative', 'negative', 'negative',\n",
    "            'negative', 'negative', 'positive', 'negative', 'neutral', 'negative', 'negative',\n",
    "            'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative',\n",
    "            'negative', 'negative', 'negative']\n",
    "\n",
    "print(metrics.confusion_matrix(dk_ws_pred, dk_actual))\n",
    "print(metrics.classification_report(dk_ws_pred, dk_actual, digits = 3))\n",
    "\n",
    "print(metrics.confusion_matrix(dk_zu_pred, dk_actual))\n",
    "print(metrics.classification_report(dk_zu_pred, dk_actual, digits = 3))\n",
    "\n",
    "print(metrics.confusion_matrix(dk_hyb_pred, dk_actual))\n",
    "print(metrics.classification_report(dk_hyb_pred, dk_actual, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the conservative source, we proceeded with the hybrid version of two dictionaries - it seemed to be an optimal choice.   \n",
    "\n",
    "#### SentiWS \n",
    "  \n",
    "|     |precision  |  recall | f1-score |  support |\n",
    "| ---|---|---|---|---|\n",
    "   negative  |    0.444   |  0.923  |   0.600    |    13\n",
    "     neutral  |    0.333   |  0.091  |   0.143    |    11\n",
    "    positive   |   0.000    | 0.000   |  0.000     |    7\n",
    "    accuracy    |            |        |  0.419     |   31\n",
    "   macro avg    |  0.259    | 0.338   |  0.248     |   31\n",
    "weighted avg    |  0.305   | 0.419   | 0.302       | 31\n",
    "\n",
    "#### PolartLexicon\n",
    "\n",
    " |         |    precision  |  recall | f1-score |  support|\n",
    " |---|---|---|---|---|\n",
    "    negative   |   0.370 |    0.909  |   0.526   |     11\n",
    "     neutral   |   0.000  |   0.000   |  0.000    |     2\n",
    "    positive    |  1.000   |  0.056   |  0.105    |    18\n",
    "    accuracy    |          |          |  0.355    |    31\n",
    "   macro avg  |    0.457   |  0.322  |   0.211    |    31\n",
    "weighted avg  |    0.712   |  0.355   |  0.248     |   31\n",
    "\n",
    "#### Hybrid\n",
    "\n",
    "|         | precision |   recall  |f1-score  | support|\n",
    "|---|---|---|---|---|\n",
    "    negative    |  0.593    | 0.889 |    0.711       | 18\n",
    "     neutral     | 0.000    | 0.000  |   0.000      |   0\n",
    "    positive     | 1.000    | 0.077   |  0.143     |   13\n",
    "    accuracy      |          |        |  0.700     |   31\n",
    "   macro avg      |0.531    | 0.322   |  0.285    |    31\n",
    "weighted avg      |0.763   |  0.548   |  0.473   |     31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making humans (us) manually validate all texts would be painful. This is why we samples around 1/5 of texts for human reading. But can we be sure that the smaller samples represent the original samples well? What if they contain mostly extreme values and skew the picture? To avoid this, we applied Student's t-test. So far so good: there is not significant difference between the two for each source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taz WS Ttest_indResult(statistic=0.23352894920570835, pvalue=0.8156419312334099)\n",
      "Taz Zu Ttest_indResult(statistic=0.3519095262971695, pvalue=0.7253578796107707)\n",
      "DK WS Ttest_indResult(statistic=0.6671381789173757, pvalue=0.5055991574300808)\n",
      "DK Zu Ttest_indResult(statistic=1.5021159239971085, pvalue=0.13494445959895884)\n",
      "Stern WS Ttest_indResult(statistic=0.8092765972524079, pvalue=0.41943650500320184)\n",
      "Stern Zu Ttest_indResult(statistic=-1.4297644923319337, pvalue=0.15453744986583487)\n"
     ]
    }
   ],
   "source": [
    "def t_test(filename, length):\n",
    "    '''this function checks whether drawing a smaller sample \n",
    "    from a big sample may skew the results'''\n",
    "    df = pd.read_excel(filename)\n",
    "    df_slice_for_testing = df.loc[:length]\n",
    "    list_for_testing_small = df_slice_for_testing['Value'].tolist()\n",
    "    list_for_testing_big = df['Value'].tolist()\n",
    "    return stats.ttest_ind(list_for_testing_small, list_for_testing_big)\n",
    "\n",
    "print('Taz WS', t_test('Taz Sentiment.xlsx', 30))\n",
    "print('Taz Zu', t_test('Taz_Sentiment_Zurich.xlsx', 30))\n",
    "\n",
    "print('DK WS', t_test('DK Sentiment.xlsx', 30))\n",
    "print('DK Zu', t_test('DK_Sentiment_Zurich.xlsx', 30))\n",
    "\n",
    "print('Stern WS', t_test('Stern_Sentiment.xlsx', 30))\n",
    "print('Stern Zu', t_test('Stern_Sentiment_Zurich.xlsx', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taz WS Ttest_indResult(statistic=0.23352894920570835, pvalue=0.8156419312334099)\n",
    "Taz Zu Ttest_indResult(statistic=0.3519095262971695, pvalue=0.7253578796107707)\n",
    "\n",
    "DK WS Ttest_indResult(statistic=0.6671381789173757, pvalue=0.5055991574300808)\n",
    "DK Zu Ttest_indResult(statistic=1.5021159239971085, pvalue=0.13494445959895884)\n",
    "\n",
    "Stern WS Ttest_indResult(statistic=0.8092765972524079, pvalue=0.41943650500320184)\n",
    "Stern Zu Ttest_indResult(statistic=-1.4297644923319337, pvalue=0.15453744986583487)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
